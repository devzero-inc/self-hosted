namespace: &namespace devzero-self-hosted

devzero:
  teamId: ""
  region: ""
  vault:
    server: "https://csi.devzero.io"

nodeLabeler:
  enabled: true

credentials:
  registry:
  username:
  password:
  email:

kube-prometheus-stack:
  prometheus:
    prometheusSpec:
      additionalScrapeConfigs:
        - job_name: "cortex"
          metrics_path: "/metrics"
          kubernetes_sd_configs:
            - role: "pod"
      podMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      remoteWrite:
        - url: "https://mimir.devzero.dev/api/v1/push"
          headers:
            X-Scope-OrgID: "1"
          writeRelabelConfigs:
            - targetLabel: "region"
              replacement: "self-hosted"

rook-ceph:
  image:
    repository: docker.io/rook/ceph
  nodeSelector:
    node-role.kubernetes.io/rook-node: "1"

  crds:
    enabled: false

  tolerations:
    - key: "rookNode"
      operator: "Exists"
      effect: "NoSchedule"

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "node-role.kubernetes.io/rook-node"
                operator: "Exists"

  csi:
    provisionerTolerations:
      - key: "rookNode"
        operator: "Exists"
        effect: "NoSchedule"

    provisionerNodeAffinity: node-role.kubernetes.io/rook-node;

rook-ceph-cluster:
  operatorNamespace: *namespace

  toolbox:
    enabled: true
    tolerations:
      - key: "rookNode"
        operator: "Exists"
        effect: "NoSchedule"

    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "node-role.kubernetes.io/rook-node"
                  operator: "Exists"

  cephClusterSpec:
    annotations:
      mgr:
        prometheus.io/port: "9283"
        prometheus.io/scrape: "true"
      exporter:
        prometheus.io/port: "9926"
        prometheus.io/scrape: "true"
    cephVersion:
      image: quay.io/ceph/ceph:v18.2.4
      allowUnsupported: false
    mon:
      count: 3
      allowMultiplePerNode: false
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: "40Gi"
    mgr:
      count: 2
      allowMultiplePerNode: false
      modules:
        - name: rook
          enabled: true
    dashboard:
      enabled: true
      ssl: true
    placement:
      all:
        tolerations:
          - key: "rookNode"
            operator: "Exists"
            effect: "NoSchedule"
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/rook-node
                    operator: In
                    values:
                      - "1"
    storage:
      useAllNodes: false
      useAllDevices: false
      storageClassDeviceSets:
        - name: set1
          count: 3
          portable: true
          tuneDeviceClass: true
          placement:
            topologySpreadConstraints:
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
          preparePlacement:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd-prepare
                    topologyKey: kubernetes.io/hostname
            topologySpreadConstraints:
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd-prepare
          volumeClaimTemplates:
            - metadata:
                name: data
              spec:
                resources:
                  requests:
                    storage: "100Gi"
                storageClassName: premium-rwo
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce

  cephBlockPools:
    - name: ceph-blockpool
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
      spec:
        failureDomain: host
        replicated:
          size: 3
      storageClass:
        enabled: true
        name: ceph-block
        annotations: { }
        labels: { }
        isDefault: false
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        mountOptions: [ ]
        allowedTopologies: [ ]
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: *namespace
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: *namespace
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: *namespace
          csi.storage.k8s.io/fstype: ext4

  cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - failureDomain: host
            replicated:
              size: 3
            name: data0
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            limits:
              memory: "4Gi"
            requests:
              cpu: "1000m"
              memory: "4Gi"
          placement:
            tolerations:
              - key: "rookNode"
                operator: "Exists"
                effect: "NoSchedule"
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: node-role.kubernetes.io/rook-node
                        operator: In
                        values:
                          - "1"
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-filesystem
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        annotations: { }
        labels: { }
        mountOptions: [ ]
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: *namespace
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: *namespace
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: *namespace
          csi.storage.k8s.io/fstype: ext4

  cephFileSystemVolumeSnapshotClass:
    enabled: false
    name: ceph-filesystem
    isDefault: false
    deletionPolicy: Delete
    annotations: { }
    labels: { }
    parameters: { }

  cephBlockPoolsVolumeSnapshotClass:
    enabled: false
    name: ceph-block
    isDefault: false
    deletionPolicy: Delete
    annotations: { }
    labels: { }
    parameters: { }
